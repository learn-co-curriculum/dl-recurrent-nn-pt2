{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NNs for sequence modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done NLP before, looking at the data sets with bank complaints. Why are \"regular' neural networks sometimes insufficient to deal with NLP and sequence-type problems?\n",
    "- We created vectors with 0s and 1s denoting if certain words are in a given bank complaint, but no information on the sequence of the word is used!\n",
    "- In this example, inputs and outputs can be different lengths for different index numbers (i)\n",
    "- General neural networks cannot learn features across different positions of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take first word $x^{<1>}$, feed it in an NN layer, and try to predict $\\hat y^{<1>}$. previous words also have an effect on the output for words later in the sequence. We use the weights $w_{ax}, w_{aa}$ and $w_{ya}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantage: networks only uses words earlier in the sequence. Eg:\n",
    "\n",
    "In \"Europe, people tend to use square meters instead of square feet.\"\n",
    "\n",
    "The \"meters\" and \"feet\" would have been useful to identify that in this case, square is not a physical location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{<0>}$ = vector of zeros\n",
    "\n",
    "$a^{<1>} = g(w_{aa}a^{<0>} +w_{ax}x^{<1>}+b_a )$, tanh or relu\n",
    "\n",
    "$\\hat y^{<1>} = g(w_{ya}a^{<1>} + b_y )$, sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpler notation:\n",
    "\n",
    "$a^{<1>} = g(w_{a}[a^{<0>},x^{<1>}]+b_a )$\n",
    "\n",
    "$\\hat y^{<t>} = g(w_{y}a^{<t>}+b_y )$\n",
    "\n",
    "Matrix $w_a=[w_{aa}; w_{ax}]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"backpropagation through time\" --> Loss function in each vertical in the image above, and then take the sum over all of them, also, right-to=left backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different types of architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many-to-many --> (location identifyer)\n",
    "- Many-to-one --> text classifyer (good vs bad review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_manytoone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-to-many --> music generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_onetomany.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- many to many, but input and output lengths are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](RNN_manytomany.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/nlp-sequence-models/lecture/gw1Xw/language-model-and-sequence-generation\n",
    "\n",
    "https://www.coursera.org/learn/nlp-sequence-models/lecture/MACos/sampling-novel-sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Issues in sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanishing gradients: plural subject, and maybe much later in the sentence we'll need a plural verb.\n",
    "- Exploding gradients: you'll notice NaNs, solution is gradient clipping (but, exploding gradients are rare in recurrent networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Solution for vanishing gradients: Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a memory cell C. A simplified GRU then looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN a GRU, $a^{<t>} = C^{<t>}$\n",
    "Next, candidate for replacing $C^{<t>}$, which is \n",
    "    \n",
    "$\\tilde C ^{<t>} = \\tanh(w_c[C^{<t-1>}, x^{<t>}]+b_c)$\n",
    "\n",
    "$\\Gamma_u = \\sigma(w_u[c^{<t-1>}, x^{<t>}]+b_u])$\n",
    "--> always 0 or 1\n",
    "\n",
    "$C ^{<t>} =\\Gamma_u * \\tilde C ^{<t>} + (1- \\Gamma_u)* C^{<t-1>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\Gamma$ is used to decide whether or not to update! If update gate is 0, $C^{<t>}$ will not be updated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how many hidden activation values you have, $C^{<t>}, \\tilde C^{<t>}$ and $\\Gamma_u$ will be vectors of values, and the last equation above is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A FULL GRU adds another gate which tells us how important $C^{<t-1>}$ for the update of $\\tilde C ^{<t>}$. The new equations are:\n",
    "\n",
    "$\\tilde C ^{<t>} = \\tanh(w_c[\\Gamma_r * C^{<t-1>}, x^{<t>}]+b_c)$\n",
    "\n",
    "$\\Gamma_u = \\sigma(w_u[c^{<t-1>}, x^{<t>}]+b_u])$\n",
    "\n",
    "$C ^{<t>} =\\Gamma_u * \\tilde C ^{<t>} + (1- \\Gamma_u)* C^{<t-1>}$\n",
    "\n",
    "$\\Gamma_r = \\sigma(w_r[c^{<t-1>}, x^{<t>}]+b_r])$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Solution for vanishing gradients: Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTM, we do not use that $a^{<t>}=C ^{<t>}$\n",
    "\n",
    "$\\tilde C ^{<t>} = \\tanh(w_c[a^{<t-1>}, x^{<t>}]+b_c)$\n",
    "\n",
    "$\\Gamma_u = \\sigma(w_u[a^{<t-1>}, x^{<t>}]+b_u])$\n",
    "\n",
    "$\\Gamma_f = \\sigma(w_u[c^{<t-1>}, x^{<t>}]+b_f])$\n",
    "\n",
    "$\\Gamma_o = \\sigma(w_o[c^{<t-1>}, x^{<t>}]+b_o])$\n",
    "\n",
    "$\\tilde C ^{<t>} = \\Gamma_u* \\tilde C ^{<t>} + \\Gamma_f* C ^{<t-1>}$ \n",
    "$a ^{<t>} = \\Gamma_o* \\tanh C ^{<t>} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\Gamma_u, \\Gamma_f$ and $\\Gamma_o$ the update, forget and output gate respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Which one to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU: simpler, and easier to run on a big network\n",
    "\n",
    "LSTM: more powerful, more flexible\n",
    "\n",
    "LSTM is generally considered as the default, but more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Other networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-directional RNN: take information about the entire sequence, not just what is preceding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep RNNs: several vertical layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
